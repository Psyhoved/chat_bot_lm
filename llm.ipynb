{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T09:39:45.407127Z",
     "start_time": "2024-09-23T09:39:39.504522Z"
    }
   },
   "source": [
    "from libs.llm_chat import (create_chain, check_question, get_session_history, MODEL)\n",
    "from main import fast_answer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### История сообщений",
   "id": "4ac7b44e541f356e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T11:15:32.742983Z",
     "start_time": "2024-09-20T11:15:31.082655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "session_history = get_session_history('саша320')\n",
    "a = session_history.messages\n",
    "a"
   ],
   "id": "c2ae9ea9a857ebac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Сколько стоит капучино?'),\n",
       " AIMessage(content='К сожалению, у меня нет информации о ценах на конкретные продукты в магазинах \"Жизньмарт\". Однако я могу помочь вам узнать о других наших акциях и товарах! Если вас интересуют еще какие-либо вопросы о нашем магазине, не стесняйтесь спрашивать!'),\n",
       " HumanMessage(content='акции на капучино'),\n",
       " AIMessage(content='У меня нет актуальной информации о текущих акциях на капучино в магазинах \"Жизньмарт\". Однако в нашем магазине часто проходят интересные предложения и скидки на различные товары. Рекомендую посетить наш сайт или спросить у сотрудников магазина для получения самой свежей информации. Если у вас есть другие вопросы о \"Жизньмарт\", я с радостью на них отвечу!')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:25:04.551935Z",
     "start_time": "2024-09-18T11:25:02.836002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "session_history = get_session_history(user_id)\n",
    "\n",
    "# Преобразование сообщений в список словарей\n",
    "messages = []\n",
    "for message in session_history.messages:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        messages.append({\"Human\": message.content})\n",
    "    elif isinstance(message, AIMessage):\n",
    "        messages.append({\"AI\": message.content})\n",
    "messages"
   ],
   "id": "ac0108f59893eec3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Human': 'как заказать воду?'},\n",
       " {'AI': 'Заказ воды можно оформить в разделе Доставка воды на нашем сайте – https://lifemart.ru/ru/ekb/water-order.'},\n",
       " {'Human': 'у меня в салате волос'},\n",
       " {'AI': 'Очень жаль слышать о вашей ситуации. Пожалуйста, обратитесь в один из наших магазинов, позвонив по номеру, указанному в разделе «Наши магазины» на сайте или в приложении. Наши супергерои с радостью помогут вам и разберутся в этом вопросе! Не забудьте указать дату изготовления и прикрепить фото, если это возможно. Мы ценим ваше время и хотим, чтобы вы остались довольны!'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Подбор промтов",
   "id": "2bb617589b90623f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from libs.llm_chat import define_hf_chat, HF_KEY\n",
   "id": "4a9c97f883bcf63d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:40:01.118933Z",
     "start_time": "2024-09-06T11:39:54.250356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from libs.llm_chat import define_promt, get_history_aware_retriever, VEC_STORE_LOAD_PATH\n",
    "\n",
    "hf_llm = define_hf_chat(api_key=HF_KEY, model='IlyaGusev/saiga_llama3_8b',\n",
    "                        temperature=0.5, max_tokens=500)\n",
    "prompt = define_promt(system_prompt)\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(hf_llm, prompt)\n",
    "history_aware_retriever = get_history_aware_retriever(hf_llm, VEC_STORE_LOAD_PATH, story_prompt)\n",
    "\n",
    "chain = create_retrieval_chain(history_aware_retriever, doc_chain)\n",
    "\n",
    "# Create a chain\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "conversational_rag_chain.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})"
   ],
   "id": "c9e284cea3054014",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: W5OYR05rye0yfPyO7v261)\n\nBad request:\nAuthorization header is correct, but the token seems invalid",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\requests\\models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/IlyaGusev/saiga_llama3_8b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 23\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Create a chain\u001B[39;00m\n\u001B[0;32m     16\u001B[0m conversational_rag_chain \u001B[38;5;241m=\u001B[39m RunnableWithMessageHistory(\n\u001B[0;32m     17\u001B[0m     chain,\n\u001B[0;32m     18\u001B[0m     get_session_history,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     output_messages_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     22\u001B[0m )\n\u001B[1;32m---> 23\u001B[0m \u001B[43mconversational_rag_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfigurable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msession_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m}\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2876\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2877\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2878\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2879\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2880\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\branch.py:239\u001B[0m, in \u001B[0;36mRunnableBranch.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    237\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 239\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    240\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    241\u001B[0m             config\u001B[38;5;241m=\u001B[39mpatch_config(\n\u001B[0;32m    242\u001B[0m                 config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(tag\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbranch:default\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m             ),\n\u001B[0;32m    244\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    245\u001B[0m         )\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    247\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2874\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, config)\n\u001B[0;32m   2875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2876\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2877\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2878\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:495\u001B[0m, in \u001B[0;36mRunnableAssign.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    489\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;28minput\u001B[39m: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    492\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    494\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m--> 495\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_with_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1785\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[1;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[0;32m   1782\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[0;32m   1783\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[0;32m   1784\u001B[0m         Output,\n\u001B[1;32m-> 1785\u001B[0m         context\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m   1786\u001B[0m             call_func_with_variable_args,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m             func,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1788\u001B[0m             \u001B[38;5;28minput\u001B[39m,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m             config,\n\u001B[0;32m   1790\u001B[0m             run_manager,\n\u001B[0;32m   1791\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1792\u001B[0m         ),\n\u001B[0;32m   1793\u001B[0m     )\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1795\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:427\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[1;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[0;32m    426\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[1;32m--> 427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:482\u001B[0m, in \u001B[0;36mRunnableAssign._invoke\u001B[1;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_invoke\u001B[39m(\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    471\u001B[0m     \u001B[38;5;28minput\u001B[39m: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    475\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m    476\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m    477\u001B[0m         \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mdict\u001B[39m\n\u001B[0;32m    478\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m--> 482\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmapper\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    483\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    484\u001B[0m             patch_config(config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child()),\n\u001B[0;32m    485\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    486\u001B[0m         ),\n\u001B[0;32m    487\u001B[0m     }\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3580\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[1;34m(self, input, config)\u001B[0m\n\u001B[0;32m   3575\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m   3576\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3577\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[0;32m   3578\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   3579\u001B[0m         ]\n\u001B[1;32m-> 3580\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: future\u001B[38;5;241m.\u001B[39mresult() \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[0;32m   3581\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3582\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3580\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   3575\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m   3576\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3577\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[0;32m   3578\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   3579\u001B[0m         ]\n\u001B[1;32m-> 3580\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[0;32m   3581\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3582\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:458\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 403\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3564\u001B[0m, in \u001B[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001B[1;34m(step, input, config, key)\u001B[0m\n\u001B[0;32m   3562\u001B[0m context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[0;32m   3563\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[1;32m-> 3564\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3565\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3566\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchild_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3568\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\branch.py:239\u001B[0m, in \u001B[0;36mRunnableBranch.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    237\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 239\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    240\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    241\u001B[0m             config\u001B[38;5;241m=\u001B[39mpatch_config(\n\u001B[0;32m    242\u001B[0m                 config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(tag\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbranch:default\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m             ),\n\u001B[0;32m    244\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    245\u001B[0m         )\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    247\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2876\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2877\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2878\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2879\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2880\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:344\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    336\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    340\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    341\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    342\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 344\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    345\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    346\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    347\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    348\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    349\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    350\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    351\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    352\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    353\u001B[0m         )\n\u001B[0;32m    354\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    355\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m    356\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:701\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    693\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    694\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    695\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    698\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    699\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    700\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 701\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_strings, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:880\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    865\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    866\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    867\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    868\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    878\u001B[0m         )\n\u001B[0;32m    879\u001B[0m     ]\n\u001B[1;32m--> 880\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[0;32m    881\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    882\u001B[0m     )\n\u001B[0;32m    883\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:738\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[0;32m    737\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 738\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    739\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    740\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:725\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    715\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[0;32m    716\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    717\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    722\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    723\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    724\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 725\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    726\u001B[0m                 prompts,\n\u001B[0;32m    727\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    728\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[0;32m    729\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    730\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    731\u001B[0m             )\n\u001B[0;32m    732\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    733\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[0;32m    734\u001B[0m         )\n\u001B[0;32m    735\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    736\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1429\u001B[0m, in \u001B[0;36mLLM._generate\u001B[1;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1426\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1427\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[0;32m   1428\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m-> 1429\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1430\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m   1431\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1432\u001B[0m     )\n\u001B[0;32m   1433\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[0;32m   1434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:139\u001B[0m, in \u001B[0;36mHuggingFaceHub._call\u001B[1;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    136\u001B[0m _model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m    137\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_model_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m--> 139\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m response \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m response:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:304\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[1;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[0;32m    301\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 304\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[0;32m    306\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:358\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n\u001B[0;32m    355\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    356\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m endpoint:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    357\u001B[0m     )\n\u001B[1;32m--> 358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[0;32m    361\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Forbidden: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCannot access content at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    364\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf you are trying to create or update content, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    365\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake sure you have a token with the `write` role.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    366\u001B[0m     )\n",
      "\u001B[1;31mBadRequestError\u001B[0m:  (Request ID: W5OYR05rye0yfPyO7v261)\n\nBad request:\nAuthorization header is correct, but the token seems invalid"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T11:44:09.715645Z",
     "start_time": "2024-09-06T11:44:08.858239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/IlyaGusev/saiga_llama3_8b\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_KEY}\"}\n",
    "\n",
    "# Пример запроса\n",
    "data = {\n",
    "    \"inputs\": \"Пример вопроса или текста для модели\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "\n",
    "# Получаем ответ\n",
    "output = response.json()\n",
    "print(output)\n"
   ],
   "id": "a85dfdbd06bb5bb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Authorization header is correct, but the token seems invalid'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Подключаем GPT\n",
   "id": "c31f00e192bf3bb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:42:52.386979Z",
     "start_time": "2024-09-23T11:42:49.197103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from libs.llm_chat import OPENAI_KEY\n",
    "\n",
    "story_prompt = \"\"\"find the documents closest to the question in meaning\n",
    "    \"\"\"\n",
    "# \"\"\"Далее представлена история сообщений данного гостя с чат-ботом. В ней может не быть правильных ответов,\n",
    "# ведь если бы человек получил правильный ответ, он бы больше не писал боту.\n",
    "# Но в ней может быть полезный для поиска правильного ответа контекст.\n",
    "#     \"\"\"\n",
    "system_prompt = \"\"\"Ты - чат-бот консультант, и работаешь в чате службы поддержки сети магазинов хороших продуктов \"Жизньмарт\",\n",
    "    твоя функция - стараться ответить на любой вопрос клиента про работу магазинов \"Жизьмарт\".\n",
    "    Ты говоришь только на чистом, грамотном русском языке без ошибок!\n",
    "    Если вопрос не касается контекста, то вежливо и дружелюбно расскажи про Жизьмарт.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Используй только этот контекст, чтобы ответить на последний вопрос.\n",
    "    Твой ответ должен быть полным и точно соответствовать тому, что написано в context.\n",
    "    Если ответа нет в контексте, просто позитивно поддержи диалог на тему Жизньмарта!\n",
    "    \"\"\"\n",
    "# Сформулируй ответ на основе того, как человек общается с ботом в context\n",
    "# если ответа на вопрос, касающийся работы магазинов Жизньмарт нет в context, ответь, что пока не готов ответить на этот вопрос, т.к. он находится в рассмотрении\n",
    "\n",
    "chain = create_chain(model=MODEL, api_key=OPENAI_KEY,\n",
    "                     system_prompt=system_prompt, story_prompt=story_prompt,\n",
    "                     temperature=1, max_tokens=500)\n",
    "chain"
   ],
   "id": "91ac28106c89a295",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F608E75600>))], default=ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='find the documents closest to the question in meaning\\n    ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F608DBB3D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F608DECF70>, model_name='gpt-4o-mini', temperature=1.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=500)\n",
       "             | StrOutputParser()\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F608E75600>)), config={'run_name': 'retrieve_documents'})\n",
       "  })\n",
       "  | RunnableAssign(mapper={\n",
       "      answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                context: RunnableLambda(format_docs)\n",
       "              }), config={'run_name': 'format_inputs'})\n",
       "              | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Ты - чат-бот консультант, и работаешь в чате службы поддержки сети магазинов хороших продуктов \"Жизньмарт\",\\n    твоя функция - стараться ответить на любой вопрос клиента про работу магазинов \"Жизьмарт\".\\n    Ты говоришь только на чистом, грамотном русском языке без ошибок!\\n    Если вопрос не касается контекста, то вежливо и дружелюбно расскажи про Жизьмарт.\\n\\n    {context}\\n\\n    Используй только этот контекст, чтобы ответить на последний вопрос.\\n    Твой ответ должен быть полным и точно соответствовать тому, что написано в context.\\n    Если ответа нет в контексте, просто позитивно поддержи диалог на тему Жизньмарта!\\n    ')), MessagesPlaceholder(variable_name='chat_history', n_messages=6), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F608DBB3D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F608DECF70>, model_name='gpt-4o-mini', temperature=1.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=500)\n",
       "              | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "    }), config={'run_name': 'retrieval_chain'}), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x000001F608D12170>]))], default=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F608E75600>))], default=ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='find the documents closest to the question in meaning\\n    ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F608DBB3D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F608DECF70>, model_name='gpt-4o-mini', temperature=1.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=500)\n",
       "             | StrOutputParser()\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F608E75600>)), config={'run_name': 'retrieve_documents'})\n",
       "  })\n",
       "  | RunnableAssign(mapper={\n",
       "      answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                context: RunnableLambda(format_docs)\n",
       "              }), config={'run_name': 'format_inputs'})\n",
       "              | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Ты - чат-бот консультант, и работаешь в чате службы поддержки сети магазинов хороших продуктов \"Жизньмарт\",\\n    твоя функция - стараться ответить на любой вопрос клиента про работу магазинов \"Жизьмарт\".\\n    Ты говоришь только на чистом, грамотном русском языке без ошибок!\\n    Если вопрос не касается контекста, то вежливо и дружелюбно расскажи про Жизьмарт.\\n\\n    {context}\\n\\n    Используй только этот контекст, чтобы ответить на последний вопрос.\\n    Твой ответ должен быть полным и точно соответствовать тому, что написано в context.\\n    Если ответа нет в контексте, просто позитивно поддержи диалог на тему Жизньмарта!\\n    ')), MessagesPlaceholder(variable_name='chat_history', n_messages=6), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F608DBB3D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F608DECF70>, model_name='gpt-4o-mini', temperature=1.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=500)\n",
       "              | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "    }), config={'run_name': 'retrieval_chain'}, config_factories=[<function RunnableBinding.with_listeners.<locals>.<lambda> at 0x000001F608D136D0>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x000001F662379510>, input_messages_key='input', output_messages_key='answer', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:41:05.112278Z",
     "start_time": "2024-09-23T11:41:01.968629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_id = 'саша333'\n",
    "question = 'к вам можно с животными?'"
   ],
   "id": "7a409ca8b385600",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:43:00.001282Z",
     "start_time": "2024-09-23T11:42:57.277508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tm(question):\n",
    "    question = check_question(question)\n",
    "    if fast_answer(question):\n",
    "        return fast_answer(question)\n",
    "    ans_dict = chain.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})\n",
    "    # ans_dict['answer'] = ans_dict['answer'].replace('\\n', '')\n",
    "    return ans_dict\n",
    "tm(question)"
   ],
   "id": "7f4c9e64625facf5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# ans_dict['answer'] = ans_dict['answer'].replace('\\n', '')\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ans_dict\n\u001B[1;32m----> 8\u001B[0m \u001B[43mtm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 5\u001B[0m, in \u001B[0;36mtm\u001B[1;34m(question)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fast_answer(question):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fast_answer(question)\n\u001B[1;32m----> 5\u001B[0m ans_dict \u001B[38;5;241m=\u001B[39m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfigurable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msession_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m}\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# ans_dict['answer'] = ans_dict['answer'].replace('\\n', '')\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ans_dict\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2876\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2877\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2878\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2879\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2880\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\branch.py:239\u001B[0m, in \u001B[0;36mRunnableBranch.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    237\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 239\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    240\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    241\u001B[0m             config\u001B[38;5;241m=\u001B[39mpatch_config(\n\u001B[0;32m    242\u001B[0m                 config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(tag\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbranch:default\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m             ),\n\u001B[0;32m    244\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    245\u001B[0m         )\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    247\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2874\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, config)\n\u001B[0;32m   2875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2876\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2877\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2878\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:495\u001B[0m, in \u001B[0;36mRunnableAssign.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    489\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;28minput\u001B[39m: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    492\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    494\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m--> 495\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_with_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1785\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[1;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[0;32m   1782\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[0;32m   1783\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[0;32m   1784\u001B[0m         Output,\n\u001B[1;32m-> 1785\u001B[0m         context\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m   1786\u001B[0m             call_func_with_variable_args,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m             func,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1788\u001B[0m             \u001B[38;5;28minput\u001B[39m,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m             config,\n\u001B[0;32m   1790\u001B[0m             run_manager,\n\u001B[0;32m   1791\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1792\u001B[0m         ),\n\u001B[0;32m   1793\u001B[0m     )\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1795\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:427\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[1;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[0;32m    426\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[1;32m--> 427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:482\u001B[0m, in \u001B[0;36mRunnableAssign._invoke\u001B[1;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_invoke\u001B[39m(\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    471\u001B[0m     \u001B[38;5;28minput\u001B[39m: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    475\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m    476\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m    477\u001B[0m         \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mdict\u001B[39m\n\u001B[0;32m    478\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m--> 482\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmapper\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    483\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    484\u001B[0m             patch_config(config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child()),\n\u001B[0;32m    485\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    486\u001B[0m         ),\n\u001B[0;32m    487\u001B[0m     }\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3580\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[1;34m(self, input, config)\u001B[0m\n\u001B[0;32m   3575\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m   3576\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3577\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[0;32m   3578\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   3579\u001B[0m         ]\n\u001B[1;32m-> 3580\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: future\u001B[38;5;241m.\u001B[39mresult() \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[0;32m   3581\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3582\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3580\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   3575\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m   3576\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   3577\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[0;32m   3578\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   3579\u001B[0m         ]\n\u001B[1;32m-> 3580\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[0;32m   3581\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   3582\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:458\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 403\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3564\u001B[0m, in \u001B[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001B[1;34m(step, input, config, key)\u001B[0m\n\u001B[0;32m   3562\u001B[0m context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[0;32m   3563\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[1;32m-> 3564\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3565\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3566\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchild_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3568\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   5088\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   5089\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5090\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   5091\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   5092\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   5093\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 5094\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   5095\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   5096\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   5097\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   5098\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\branch.py:229\u001B[0m, in \u001B[0;36mRunnableBranch.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    220\u001B[0m     expression_value \u001B[38;5;241m=\u001B[39m condition\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    222\u001B[0m         config\u001B[38;5;241m=\u001B[39mpatch_config(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    225\u001B[0m         ),\n\u001B[0;32m    226\u001B[0m     )\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m expression_value:\n\u001B[1;32m--> 229\u001B[0m         output \u001B[38;5;241m=\u001B[39m runnable\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    230\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    231\u001B[0m             config\u001B[38;5;241m=\u001B[39mpatch_config(\n\u001B[0;32m    232\u001B[0m                 config,\n\u001B[0;32m    233\u001B[0m                 callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(tag\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbranch:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    234\u001B[0m             ),\n\u001B[0;32m    235\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    236\u001B[0m         )\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2876\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2877\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2878\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2879\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2880\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\retrievers.py:251\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    250\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_retriever_error(e)\n\u001B[1;32m--> 251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_retriever_end(\n\u001B[0;32m    254\u001B[0m         result,\n\u001B[0;32m    255\u001B[0m     )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\retrievers.py:244\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    242\u001B[0m _kwargs \u001B[38;5;241m=\u001B[39m kwargs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expects_other_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_arg_supported:\n\u001B[1;32m--> 244\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\n\u001B[0;32m    245\u001B[0m         \u001B[38;5;28minput\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs\n\u001B[0;32m    246\u001B[0m     )\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    248\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs)\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1040\u001B[0m, in \u001B[0;36mVectorStoreRetriever._get_relevant_documents\u001B[1;34m(self, query, run_manager)\u001B[0m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_relevant_documents\u001B[39m(\n\u001B[0;32m   1037\u001B[0m     \u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m, run_manager: CallbackManagerForRetrieverRun\n\u001B[0;32m   1038\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[0;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 1040\u001B[0m         docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search(query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs)\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_score_threshold\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1042\u001B[0m         docs_and_similarities \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1043\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search_with_relevance_scores(\n\u001B[0;32m   1044\u001B[0m                 query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs\n\u001B[0;32m   1045\u001B[0m             )\n\u001B[0;32m   1046\u001B[0m         )\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:530\u001B[0m, in \u001B[0;36mFAISS.similarity_search\u001B[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001B[0m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity_search\u001B[39m(\n\u001B[0;32m    511\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    512\u001B[0m     query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    517\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[0;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return docs most similar to query.\u001B[39;00m\n\u001B[0;32m    519\u001B[0m \n\u001B[0;32m    520\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;124;03m        List of Documents most similar to the query.\u001B[39;00m\n\u001B[0;32m    529\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 530\u001B[0m     docs_and_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimilarity_search_with_score(\n\u001B[0;32m    531\u001B[0m         query, k, \u001B[38;5;28mfilter\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m, fetch_k\u001B[38;5;241m=\u001B[39mfetch_k, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    532\u001B[0m     )\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [doc \u001B[38;5;28;01mfor\u001B[39;00m doc, _ \u001B[38;5;129;01min\u001B[39;00m docs_and_scores]\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:403\u001B[0m, in \u001B[0;36mFAISS.similarity_search_with_score\u001B[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return docs most similar to query.\u001B[39;00m\n\u001B[0;32m    387\u001B[0m \n\u001B[0;32m    388\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001B[39;00m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    402\u001B[0m embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_query(query)\n\u001B[1;32m--> 403\u001B[0m docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimilarity_search_with_score_by_vector(\n\u001B[0;32m    404\u001B[0m     embedding,\n\u001B[0;32m    405\u001B[0m     k,\n\u001B[0;32m    406\u001B[0m     \u001B[38;5;28mfilter\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m,\n\u001B[0;32m    407\u001B[0m     fetch_k\u001B[38;5;241m=\u001B[39mfetch_k,\n\u001B[0;32m    408\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    409\u001B[0m )\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m docs\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:304\u001B[0m, in \u001B[0;36mFAISS.similarity_search_with_score_by_vector\u001B[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_normalize_L2:\n\u001B[0;32m    303\u001B[0m     faiss\u001B[38;5;241m.\u001B[39mnormalize_L2(vector)\n\u001B[1;32m--> 304\u001B[0m scores, indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfetch_k\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    305\u001B[0m docs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\faiss\\class_wrappers.py:329\u001B[0m, in \u001B[0;36mhandle_Index.<locals>.replacement_search\u001B[1;34m(self, x, k, params, D, I)\u001B[0m\n\u001B[0;32m    327\u001B[0m n, d \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    328\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mascontiguousarray(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 329\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m d \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md\n\u001B[0;32m    331\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m k \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m D \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:10:51.435372Z",
     "start_time": "2024-09-20T12:10:46.823625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tm(question):\n",
    "    question = check_question(question)\n",
    "    if fast_answer(question):\n",
    "        return fast_answer(question)\n",
    "    ans_dict = chain.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})\n",
    "    ans_dict['answer'] = ans_dict['answer'].replace('\\n', '')\n",
    "    return ans_dict\n",
    "tm(question)"
   ],
   "id": "5cc2cc1a5c0f6ad1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'А с собачками к вам можно?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='Человек:к вам можно с собаками? Пустите с пёсиком в магазин?  \\nБот: Да, мы pet -friedly и любим, когда вы приходите к нам в гости с вашими питомцами!'),\n",
       "  Document(page_content='Человек: Почему не отображается статус заказа в личном кабинете?  \\nБот: Попробуйте свернуть приложение и вновь его о ткрыть. Перезагрузка обычно \\nпомогает 💚'),\n",
       "  Document(page_content='Человек: Как рассчитывается стоимость доставки?  \\nБот: Стоимость доставки рассчитывается автоматически в зависимости от адреса и \\nвремени заказа.  В пешей зоне (до 1 км от магазина) доставка бесплатная  с 9:00 до \\n21:00.  Если адрес доставки удален от магазина, то заказ будет доставлен оператором \\nтакси от двери до двери. Стоимость такси посчитает перевозчик. А наша умная \\nсистема подберет перевозчика с самым выгодным предложением.'),\n",
       "  Document(page_content='Человек: Как связаться с курьером?  \\nБот: В приложении в разделе «Наши мага зины» и на сайте в разделе «Адреса \\nмагазинов кафе» представлены все номера магазинов – вы можете позвонить \\nнапрямую супергероям, а они сориентируют вас по коммуникации с курьером :)')],\n",
       " 'answer': 'Да, мы pet-friendly и любим, когда вы приходите к нам в гости с вашими питомцами!'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### БЕСПЛАТНЫЕ МОДЕЛЬКИ",
   "id": "deb1122b68c1ee46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T09:43:14.432512Z",
     "start_time": "2024-09-06T09:43:10.382303Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D55572B880>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Учитывая историю чата и последний вопрос пользователя,\\n    который может ссылаться на контекст в истории чата, сформулируй отдельный вопрос,\\n    который можно понять без истории чата. НЕ ОТВЕЧАЙ на вопрос, просто переформулируйте его,\\n    если необходимо, и в противном случае верните его как есть.\\n    ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D555710340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D5557136D0>, model_name='qwen/qwen-2-7b-instruct:free', temperature=0.5, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1', openai_proxy='', max_tokens=500)\n",
       "             | StrOutputParser()\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D55572B880>)), config={'run_name': 'retrieve_documents'})\n",
       "  })\n",
       "  | RunnableAssign(mapper={\n",
       "      answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                context: RunnableLambda(format_docs)\n",
       "              }), config={'run_name': 'format_inputs'})\n",
       "              | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Ты - чат-бот консультант, и работаешь в чате службы поддержки сети магазинов хороших продуктов \"Жизньмарт\",\\n    твоя функция - стараться ответить на любой вопрос клиента про работу магазинов \"Жизьмарт\".\\n    Ты говоришь только на чистом, грамотном русском языке без ошибок!\\n    Если вопрос не касается контекста, то вежливо и дружелюбно расскажи про Жизьмарт.\\n\\n    {context}\\n\\n    Используй только этот контекст, чтобы ответить на последний вопрос.\\n    Твой ответ должен быть полным и точно соответствовать тому, что написано в context.\\n    Если ответа нет в контексте, просто позитивно поддержи диалог на тему Жизньмарта!\\n     ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D555710340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D5557136D0>, model_name='qwen/qwen-2-7b-instruct:free', temperature=0.5, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1', openai_proxy='', max_tokens=500)\n",
       "              | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "    }), config={'run_name': 'retrieval_chain'}), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x000001D5557DD990>]))], default=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D55572B880>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Учитывая историю чата и последний вопрос пользователя,\\n    который может ссылаться на контекст в истории чата, сформулируй отдельный вопрос,\\n    который можно понять без истории чата. НЕ ОТВЕЧАЙ на вопрос, просто переформулируйте его,\\n    если необходимо, и в противном случае верните его как есть.\\n    ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D555710340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D5557136D0>, model_name='qwen/qwen-2-7b-instruct:free', temperature=0.5, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1', openai_proxy='', max_tokens=500)\n",
       "             | StrOutputParser()\n",
       "             | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D55572B880>)), config={'run_name': 'retrieve_documents'})\n",
       "  })\n",
       "  | RunnableAssign(mapper={\n",
       "      answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                context: RunnableLambda(format_docs)\n",
       "              }), config={'run_name': 'format_inputs'})\n",
       "              | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Ты - чат-бот консультант, и работаешь в чате службы поддержки сети магазинов хороших продуктов \"Жизньмарт\",\\n    твоя функция - стараться ответить на любой вопрос клиента про работу магазинов \"Жизьмарт\".\\n    Ты говоришь только на чистом, грамотном русском языке без ошибок!\\n    Если вопрос не касается контекста, то вежливо и дружелюбно расскажи про Жизьмарт.\\n\\n    {context}\\n\\n    Используй только этот контекст, чтобы ответить на последний вопрос.\\n    Твой ответ должен быть полным и точно соответствовать тому, что написано в context.\\n    Если ответа нет в контексте, просто позитивно поддержи диалог на тему Жизньмарта!\\n     ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D555710340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D5557136D0>, model_name='qwen/qwen-2-7b-instruct:free', temperature=0.5, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1', openai_proxy='', max_tokens=500)\n",
       "              | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "    }), config={'run_name': 'retrieval_chain'}, config_factories=[<function RunnableBinding.with_listeners.<locals>.<lambda> at 0x000001D5557DD6C0>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x000001D52D0000D0>, input_messages_key='input', output_messages_key='answer', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "source": [
    "story_prompt = \"\"\"Учитывая историю чата и последний вопрос пользователя,\n",
    "    который может ссылаться на контекст в истории чата, сформулируй отдельный вопрос,\n",
    "    который можно понять без истории чата. НЕ ОТВЕЧАЙ на вопрос, просто переформулируйте его,\n",
    "    если необходимо, и в противном случае верните его как есть.\n",
    "    \"\"\"\n",
    "# \"\"\"Далее представлена история сообщений данного гостя с чат-ботом. В ней может не быть правильных ответов,\n",
    "# ведь если бы человек получил правильный ответ, он бы больше не писал боту.\n",
    "# Но в ней может быть полезный для поиска правильного ответа контекст.\n",
    "#     \"\"\"\n",
    "system_prompt = \"\"\"Ты - чат-бот консультант, и работаешь в чате службы поддержки сети магазинов хороших продуктов \"Жизньмарт\",\n",
    "    твоя функция - стараться ответить на любой вопрос клиента про работу магазинов \"Жизьмарт\".\n",
    "    Ты говоришь только на чистом, грамотном русском языке без ошибок!\n",
    "    Если вопрос не касается контекста, то вежливо и дружелюбно расскажи про Жизьмарт.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Используй только этот контекст, чтобы ответить на последний вопрос.\n",
    "    Твой ответ должен быть полным и точно соответствовать тому, что написано в context.\n",
    "    Если ответа нет в контексте, просто позитивно поддержи диалог на тему Жизньмарта!\n",
    "     \"\"\"\n",
    "# Сформулируй ответ на основе того, как человек общается с ботом в context\n",
    "# если ответа на вопрос, касающийся работы магазинов Жизньмарт нет в context, ответь, что пока не готов ответить на этот вопрос, т.к. он находится в рассмотрении\n",
    "\n",
    "chain = create_chain(model=qwen2,\n",
    "                     system_prompt=system_prompt, story_prompt=story_prompt,\n",
    "                     temperature=0.5, max_tokens=500)\n",
    "chain"
   ],
   "id": "8f64ab78acc7bf34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T09:43:35.877758Z",
     "start_time": "2024-09-06T09:43:29.381529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tm(question):\n",
    "    question = check_question(question)\n",
    "    if fast_answer(question):\n",
    "        return fast_answer(question)\n",
    "    ans_dict = chain.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}}) \n",
    "    ans_dict['answer'] = ans_dict['answer'].replace('\\n', '')\n",
    "    return ans_dict\n",
    "tm(question)"
   ],
   "id": "49b98e5b81e6d3d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainPendingDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `PostgresChatMessageHistory` implementation in `langchain_postgres`. Please do not submit further PRs to this class.See <https://github.com/langchain-ai/langchain-postgres>\n",
      "  warn_deprecated(\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'заказа нет час никто не берет телефон',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='Человек: Как сделать заказ?  \\nБот: Оформить заказ можно в нашем приложении или н а сайте, добавив все нужное в \\nкорзину 💚'),\n",
       "  Document(page_content='Человек: Почему не отображается статус заказа в личном кабинете?  \\nБот: Попробуйте свернуть приложение и вновь его о ткрыть. Перезагрузка обычно \\nпомогает 💚'),\n",
       "  Document(page_content='Человек : Где мой заказ?  \\nБот: Добрый день! Приносим тысячу извинений, что задерживаемся. Уточнить \\nместоположение курьера можно напрямую в магазине – после оформления заказа \\nпоявляется кнопка \"Связаться с нами\" – это прямой номер филиала.'),\n",
       "  Document(page_content='Человек: Отменили заказ а денег нет  \\nБот: Ден ежные средства за отмененный заказ возвращаются на счет в зависимости от \\nусловий вашего банка – регламент от 1 до 30 рабочих дней, однако на практике \\nвозврат приходит в течение нескольких дней. Пожалуйста, ожидайте')],\n",
       " 'answer': 'Мы понимаем вашу нервность и разделяем вашу обеспокоенность. Если вы не можете связаться с нашим магазином, я бы посоветовал вам проверить номер телефона на наличие ошибок и попытаться позвонить снова позже. Если проблема сохраняется, пожалуйста, напишите нам через наш чат поддержки, и мы с радостью поможем вам в решении проблемы. Надеемся, что всё будет很快恢复正常. В любом случае, \"Жизньмарт\" всегда стремится обеспечить вам лучший сервис.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T12:03:41.781239Z",
     "start_time": "2024-09-02T12:03:28.975153Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})",
   "id": "3e539f5c5876230a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Курьеранет сколько ждать еще?',\n",
       " 'chat_history': [HumanMessage(content='Курьеранет сколько ждать еще?'),\n",
       "  AIMessage(content='Извините, я не могу отследить местоположение курьера. Но если у вас есть номер телефона курьера, вы можете позвонить ему и уточнить.')],\n",
       " 'context': [Document(page_content='Гость: Как связаться с курьером?  \\nБот: Здравствуйте! В приложении в разделе «Наши магазины» и на сайте в разделе \\n«Адреса магазинов кафе» представлены все номера магазинов – вы можете \\nпозвонить напрямую супергероям, а они сориентируют вас по коммуникации с \\nкурьером :)'),\n",
       "  Document(page_content='Гость: Как оставить чаевые курьеру?  \\nБот: Здравствуйте! После завершения  заказа есть кнопка оставить чаевые курьеру, \\nгде вы указываете нужную сумму и переводите благодарность нашему супергерою! :)'),\n",
       "  Document(page_content='Гость: Как сменить данные в профиле?  \\nБот: Здравствуйте! Для этого перейдите в профиль в приложении или на сайте в \\nЛичный Кабинет и внесите нужные изменения. После чего сохраните.'),\n",
       "  Document(page_content='Гость: Как изменить магазин для доставки?  \\nБот: Здравствуйте! Магазин вручную выбрать нельзя. Приложение автоматически \\nформирует ваш заказ на ближайшем  магазине с доступным сервисом доставки, \\nучитывая выбран ный ассортимент в корзине.')],\n",
       " 'answer': 'Здравствуйте! Я понимаю ваше нетерпение, но к сожалению, не могу дать точную информацию о времени ожидания курьера. В приложении или на сайте Жизньмарта вы можете отслеживать статус вашего заказа. Если время доставки значительно затянулось, пожалуйста, свяжитесь с нашим магазином по номеру, указанному на сайте или в приложении, и наши супергерои помогут вам. Надеемся, ваш заказ скоро будет у вас!'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T13:20:55.705836Z",
     "start_time": "2024-08-29T13:20:55.600451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "\n",
    "user_id = 'саша102'\n",
    "# user_id = str(uuid.uuid4())\n",
    "question = 'Курьеранет сколько ждать еще?'\n",
    "# user_id"
   ],
   "id": "fd0bcc68d687d02e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T13:15:34.594225Z",
     "start_time": "2024-08-29T13:15:31.002130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_content = chain_gigachat.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})\n",
    "response_content"
   ],
   "id": "7fea27118f1db6c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Курьеранет сколько ждать еще?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='Гость: Как связаться с курьером?  \\nБот: Здравствуйте! В приложении в разделе «Наши магазины» и на сайте в разделе \\n«Адреса магазинов кафе» представлены все номера магазинов – вы можете \\nпозвонить напрямую супергероям, а они сориентируют вас по коммуникации с \\nкурьером :)'),\n",
       "  Document(page_content='Гость: Как оставить чаевые курьеру?  \\nБот: Здравствуйте! После завершения  заказа есть кнопка оставить чаевые курьеру, \\nгде вы указываете нужную сумму и переводите благодарность нашему супергерою! :)'),\n",
       "  Document(page_content='Гость: Какой кофе мне могут сделать бесплатно по карточке?  \\nБот: Здравствуйте! На главном экране в приложен ии Жизньмарт  появился раздел \\nКофе в подарок, в котором есть копилка. Нажав на акцию с кофе, вы увидите, какие \\nнапитки можно получить бесплатно 💚'),\n",
       "  Document(page_content='Гость: Спасибо, понятно/ До свидания/ решение проблемы и завершение диалога  \\nБот: Спасибо за ваше обращение в Живой Чат. Желаем вам всего самого доброго! 💚')],\n",
       " 'answer': 'Здравствуйте! К сожалению, я не могу знать, когда именно приедет курьер. Но вы всегда можете отследить его местоположение в приложении или позвонить в магазин, чтобы уточнить информацию.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T13:21:13.130956Z",
     "start_time": "2024-08-29T13:21:08.321878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_content = chain_gigachat.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})\n",
    "response_content"
   ],
   "id": "196d70e29ac5ec87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Как связаться с курьером?',\n",
       " 'chat_history': [HumanMessage(content='Курьеранет сколько ждать еще?'),\n",
       "  AIMessage(content='Здравствуйте! К сожалению, я не могу знать, когда именно приедет курьер. Но вы всегда можете отследить его местоположение в приложении или позвонить в магазин, чтобы уточнить информацию.')],\n",
       " 'context': [Document(page_content='Гость: Как связаться с курьером?  \\nБот: Здравствуйте! В приложении в разделе «Наши магазины» и на сайте в разделе \\n«Адреса магазинов кафе» представлены все номера магазинов – вы можете \\nпозвонить напрямую супергероям, а они сориентируют вас по коммуникации с \\nкурьером :)'),\n",
       "  Document(page_content='Гость: Как сдела ть заказ?  \\nБот: Здравствуйте! 💚 Оформить заказ можно в нашем приложении или на сайте, \\nдобавив все нужное в корзину.'),\n",
       "  Document(page_content='Гость: Где мой заказ?  \\nБот: Добрый день! Приносим тысячу извинений, что задерживаемся. Уточнить \\nместоположение курьера можно напрямую в магазине – после оформления заказа \\nпоявляется кнопка \"Связаться с нами\" – это прямой номер филиала.'),\n",
       "  Document(page_content='Гость: Можно выслать чек за покупку  \\nБот: Здравствуйте! Чек приходит на электронную почту при оплате заказа на доставку \\nи самовывоз. При оплате покупки на кассе магазина чек выдается бумажный. Для того, \\nчтобы получить чек с покупки в зале, вам необходим о обратиться в магазин к \\nсотрудникам, где вы совершали покупку. Наши супергерои помогут вам! :)')],\n",
       " 'answer': 'Вы можете связаться с курьером через приложение или сайт. В разделе \"Наши магазины\" или \"Адреса магазинов кафе\" указаны все номера магазинов.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T06:53:44.259446Z",
     "start_time": "2024-08-30T06:53:37.299146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_id = 'саша103'\n",
    "question = 'Курьеранет сколько ждать еще?'\n",
    "\n",
    "response_content = chain_gigachat.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})\n",
    "response_content"
   ],
   "id": "35fd4e041a1c68cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\D\\livemart\\chat_bot_lm\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainPendingDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `PostgresChatMessageHistory` implementation in `langchain_postgres`. Please do not submit further PRs to this class.See <https://github.com/langchain-ai/langchain-postgres>\n",
      "  warn_deprecated(\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Как связаться с курьером?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='Гость: Как связаться с курьером?  \\nБот: Здравствуйте! В приложении в разделе «Наши магазины» и на сайте в разделе \\n«Адреса магазинов кафе» представлены все номера магазинов – вы можете \\nпозвонить напрямую супергероям, а они сориентируют вас по коммуникации с \\nкурьером :)'),\n",
       "  Document(page_content='Гость: Как оставить чаевые курьеру?  \\nБот: Здравствуйте! После завершения  заказа есть кнопка оставить чаевые курьеру, \\nгде вы указываете нужную сумму и переводите благодарность нашему супергерою! :)'),\n",
       "  Document(page_content='Гость: Как отменить заказ?  \\nБот: Здравствуйте! После оформления заказа есть активная кнопка \"Связаться с нами\" \\n– вы увидите номер магазина, по которому можете обратиться напрямую к \\nсупергероям и попросить отменить заказ :)'),\n",
       "  Document(page_content='Гость: Как изменить магазин для доставки?  \\nБот: Здравствуйте! Магазин вручную выбрать нельзя. Приложение автоматически \\nформирует ваш заказ на ближайшем  магазине с доступным сервисом доставки, \\nучитывая выбран ный ассортимент в корзине.')],\n",
       " 'answer': 'Здравствуйте! Вы можете связаться с курьером, позвонив напрямую в магазин \"Жизньмарт\". В приложении или на сайте есть раздел \"Наши магазины\", где указаны все номера телефонов.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T06:54:51.568753Z",
     "start_time": "2024-08-30T06:54:45.245724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_id = 'саша103'\n",
    "question = 'Курьеранет сколько ждать еще?'\n",
    "\n",
    "response_content = chain_gigachat.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": user_id}})\n",
    "response_content"
   ],
   "id": "d696f6f7acb6531d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('answer')\n",
      "Error in callback coroutine: KeyError('answer')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Курьеранет сколько ждать еще?',\n",
       " 'chat_history': [HumanMessage(content='Как связаться с курьером?'),\n",
       "  AIMessage(content='Здравствуйте! Вы можете связаться с курьером, позвонив напрямую в магазин \"Жизньмарт\". В приложении или на сайте есть раздел \"Наши магазины\", где указаны все номера телефонов.')],\n",
       " 'context': [Document(page_content='Гость: Где можно оставить отзыв?  \\nБот: Здравствуйте! Отзыв можем принять в Живом чате – просто отправьте слово \\n\"оператор\", а затем опишите ваш отзыв, а также укажите магазин, где вы делали \\nпокупку, и наш оператор поможет вам :)'),\n",
       "  Document(page_content='Гость: Не могу оплатить продукцию живчик ами. \\nБот: Здравствуйте! :) Живчики можно обменять на любой товар из Живчикмарт, \\nдобавив все нужное в корзину приложения и оформив заказ на доставку или \\nсамовывоз (магазин живчиков находится в главном меню в приложении).'),\n",
       "  Document(page_content='Гость: Почему в магазине нет нужного товара  \\nБот: Здравствуйте! Ежедневно мы принимаем поставки свежей продукции на наших \\nмагазинах, вероятно, товар раскупили, и скоро он появится на полках вновь! О \\nналичии конкретного товара вы можете уточнить напрямую в нужном магазине сети – \\nномера представлены на сайте и в приложении.'),\n",
       "  Document(page_content='Гость: Как изменить магазин для доставки?  \\nБот: Здравствуйте! Магазин вручную выбрать нельзя. Приложение автоматически \\nформирует ваш заказ на ближайшем  магазине с доступным сервисом доставки, \\nучитывая выбран ный ассортимент в корзине.')],\n",
       " 'answer': 'Здравствуйте! К сожалению, я не могу точно сказать, сколько времени потребуется курьеру, чтобы доставить ваш заказ. Однако, если у вас возникли вопросы по поводу доставки, вы всегда можете обратиться в службу поддержки клиентов магазина \"Жизньмарт\".'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Комментарий по ответу:\n",
    "\n",
    "'answer': 'Здравствуйте! К сожалению, я не могу точно сказать, сколько времени потребуется курьеру, чтобы доставить ваш заказ. Однако, если у вас возникли вопросы по поводу доставки, вы всегда можете обратиться в службу поддержки клиентов магазина \"Жизньмарт\".'\n",
    "\n",
    "Ты уже чат-бот службы поддержки клиентов сети магазинов \"Жизньмарт\"! Куда ты направляешь людей, они уже к тебе обратились. И зачем ставить точку в конце каждого предложения? Это пассивно-агрессивно...\n",
    "Нужно менять системные промты"
   ],
   "id": "9a7b90f10c6c2026"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
